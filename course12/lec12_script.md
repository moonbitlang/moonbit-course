# 现代编程思想：案例：自动微分

大家好，欢迎来到由IDEA研究院基础软件中心为大家带来的现代编程思想公开课。今天我们继续案例学习，主题是自动微分。我会避开一些复杂的数学概念。

微分是一个计算机中重要的操作。机器学习中，基于梯度下降的神经网络，就会应用到微分，寻找局部最小值进行训练。而大家可能更加熟悉的是通过牛顿迭代法寻找函数的零点的近似解，这里简单复习一下。我们在这里绘制了一个函数。设初始值为1，也就是数轴上的点A。我们希望找到它附近的零点。我们计算出这个点横坐标所对应的函数上的点B，并且求出这一个点的导数，导数就是这一点的切线斜率。根据过这一点的切线与x轴的交点，我们可以获得一个更接近零点的值。之后，我们再次重复这个过程：找到函数对应的点，求出导数，找到切线与x轴的交点。这样，我们就能逐步逼近零点，获得近似解。我们将在最后给出代码的实现。

我们今天在这里只研究简单的函数组合，也就是只涉及相加和相乘。例如，计算5乘以x0的平方加上x1。如果x0为10，x1为100，那么我们需要计算的是，函数的值，600，关于x0的偏微分，100，关于x1的偏微分，1。

函数微分有几种方式。一种是手动微分，一张纸，一支笔，纯天然计算器。缺点是，对于复杂的表达式容易出错，而且不能一天24小时计算。另一种方式是进行数值微分，也就是对于想要微分的点，加一个小量，计算差值后除以小量。这个问题在于，计算机无法精准表达小数，而且绝对值越大越不精准，以及我们无法完全求解无穷级数。第三种是符号微分，也就是将函数转换为一棵表达式树，再对表达式树进行操作求出对应的导数。例如，这里的常数2乘以x，就会被求导为常数2。符号微分的问题在于，计算的结果不一定能化简足够，可能会有重复计算。另外，就是无法利用原生的控制流，例如条件判断与循环等。如果想要定义如下方的求更大值，那么不得不定义一个算子，而不能简单地对当前值进行判断。最后则是自动微分。自动微分利用复合函数的求导法则，由基本运算组合进行计算与微分，这也符合模块化的思想。自动微分分为前向微分和后向微分。我们之后将会逐一介绍。

我们首先还是先来看一下符号微分。我们利用枚举类型定义表达式。表达式可以是一个常数，也可以是一个变量，这里用整数从零开始给变量编号，也可以是两个函数相加或者两个函数相减。我们这里定义简单的构造器并且重载运算符，方便写出更简洁的表达式。最后第15行，我们利用模式匹配定义一个根据符号计算函数值的方法，输入是一个向量，在此略过。我们这里复习一下函数的求导法则，如果一个函数是常值函数，那么导数为0。如果一个变量对它自己求偏微分，那么导数为1，否则就像常数一样，导数为0。两个函数的和的导数是两个函数的导数的和，而两个函数的乘积的导数，则是函数求导分别乘以另一个函数以后相加，f的导数乘以g加上g的导数乘以f。我们在这里根据法则，利用模式匹配，对我们的符号进行求导操作。因为是偏微分，因此我们的参数还有一个下标来注明我们是对哪个变量求导。

我们利用先前数据的定义，构造我们的例子中的函数。可以看到，我们这里的乘法和加法写得非常自然，这是由于月兔允许我们重载部分运算符。我们在构造了表达式后，对它进行微分，求出对应的表达式，如第7行所示，然后我们再根据输入计算出对应的偏微分。其中，我们求出的导数的表达式在没有化简的情况下可能长得如下方所示，十分的复杂。

当然，我们也可以定义一些化简函数，或者对构造函数进行修改来简化函数。例如，这里我们对加法所得到的结果进行一些化简。0加上任何数都是那个数本身，因此我们可以只保留那个数；如果两个数相加，那么我们可以将它们化简，再和其他的变量进行计算；最后，如果我们发现整数在右侧，我们调整到左侧，这样就不用把每条优化规则写两边。

类似的，我们还可以对乘法也做这样的化简。0乘以任何数都是0，1乘以任何数都是它本身，两个数相乘可以化简，等等。这样化简以后，我们就获得了比较精简的结果。当然，这只是因为我们的例子比较简单，实际上这样的化简还有很多要做的，例如合并同类型等等。

接下来让我们看一下自动微分。我们首先通过接口定义我们想要实现的运算，包括从常数构造、加法和乘法。我们同时也希望能够获得当前的计算值。通过这个接口，我们可以利用语言的原生的控制流进行计算，动态地生成计算图。例如在下方，我们可以根据y当前的值选择一个表达式进行计算，而当我们微分的时候，也会对对应的表达式进行微分。

我们首先从前向微分开始。前向微分比较简单，直接利用求导法则进行计算，同时计算f(a)与f'(a)。之所以计算这两项而不是仅仅计算微分的简单原因是，当我们计算两个函数相乘的导数的时候，我们需要获得两个函数的当前值计算，因此我们需要同时计算值与微分。而从数学的角度上来说，它们对应线性代数中的二元数。感兴趣的同学可以自行了解。我们构造包含二元数的结构体，一个字段是当前值，而另一个字段则是当前节点的微分。从常数进行构造十分简单，值就是常数，微分为零。获取当前值也十分简单，只需要获取对应字段就行。我们这里添加一个辅助函数，对于一个变量，我们除了需要它的值以外，还需要知道这是不是我们所需要进行微分的变量，如果是，那么它的微分就是1，否则就是0，这点前面已经介绍过了。

之后，我们定义相加与相乘，根据求导法则直接计算微分。例如，两个函数f与g相加所获得的函数，值就是函数的值相加，微分也是函数的微分相加，如第4行所示。而两个函数f与g相乘所获得的函数，值就是函数的值相乘，而微分则如之前所介绍，f * g' + g * f'。这样我们就在没有构造任何中间数据结构的情况下直接计算了微分。

最后，我们利用刚才定义的带有条件表达式的例子计算微分。需要注意的是，前向微分每次只能对一个输入的参数计算微分，因此适用于输出参数多于输入参数的情况。而在神经网络中，我们则通常是有大量的参数，和一个输出。因此就要用到接下来介绍的后向微分。

后向微分是利用链式法则进行计算的。假设我们有一个函数w，是关于x y z等的，而x y z等又是关于t的函数，那么w关于t的偏微分就是w关于x的偏微分乘以x关于t的偏微分加上w关于y的偏微分乘以y关于t的偏微分，加上w关于z的偏微分乘以z关于t的偏微分，等等。例如下面的f(x0, x1) = x0平方乘以x1。我们可以看作是f关于g和h的函数，而g和h由分别是x0平方和x1。我们对每一个组成进行偏微分：f关于g的偏微分是h，关于h的偏微分是g，g关于x0的偏微分是2x0，h关于x0的偏微分是0。最后，我们利用链式法则进行组合，获得结果2x0x1。后向微分便是这样的过程，我们从最后的f关于f的偏微分开始，向后计算f关于中间函数的偏微分，以及中间函数关于中间函数的偏微分，直到中间函数关于输出参数的偏微分为止。这样做，我们逆着构造f的计算图，可以计算出每个输入节点的微分。这适用于输入参数多于输出参数的情况。

我们这里展示一个月兔的实现。后向微分的节点包括当前节点的计算值，以及一个后向的函数。这个后向的函数将利用从结果到当前节点为止累积的微分，也就是参数，更新所有构造当前节点的参数的微分。例如下方，我们定义一个代表输入的节点。我们利用一个Ref来对所有路径求出的微分进行累加。这里的后向过程已经到底，我们将函数关于当前变量的偏微分添加到累加器当中。这个偏微分只是计算图中的一条路径的偏微分。而对于常数而言，它没有任何的输入参数，因此它的后向函数什么也不做。

之后我们考虑加法与乘法。假设参与计算的函数是g与h，当前函数为f，最终计算结果为y，参数为x。那么f关于g与h的偏微分之前提过了，这里略去。最终的y关于x的偏微分中，经由f与g的路径的偏微分，是由y关于f的偏微分乘以f关于g的偏微分乘以g关于x的偏微分。其中，y关于f的偏微分对应后向函数的参数diff。因此我们可以看到第4行，我们传给g的参数是diff * 1.0，也就对应y关于f的偏微分乘以f关于g的偏微分。同样的，我们传类似的参数给h。而在第11行，根据求导法则，传给g的参数是diff乘以h的当前值，而传给h的参数则是diff乘以g的当前值。

最后我们看一下如何使用。我们构造两个Ref分别存储x与y的微分。我们利用两个累加器构造出两个输入节点，输入值分别为10和100。之后，我们利用刚才的例子进行计算，并且在正向计算完成之后调用后向函数。参数1.0对应的是f关于f的微分。此时，两个Ref中的值被更新，我们同时获得了所有输入参数的微分。

在有了后向微分后，我们完全可以尝试手写神经网络。因为时间关系，这里只展示一下利用自动微分和牛顿迭代法求零点。我们在这里利用接口定义出我们一开始所看到的函数。

之后，我们利用牛顿迭代法进行求值。因为只有一个参数，我们就利用前向微分。我们定义x为迭代的变量，初始值为1.0。因为x是我们所微分的对象，因此第二个参数为true。之后我们定义一个死循环。第5行，我们计算x所对应的函数的值与微分。第6行，如果值除以微分，也就是我们所要逼近的步长足够小，这就说明我们已经十分接近零点，我们就停止循环。第7行，在不满足条件的情况下，我们更新x的值为之前的值减去值除以微分，之后再次循环。如此，我们最后便能得出一个近似解。

总结，本章节介绍了自动微分的概念。我们展示了符号微分，也展示了自动微分的两种不同的实现。感兴趣的同学可以参考3Blue1Brown的深度学习系列，包括梯度下降法和反向传播法，尝试自己手写神经网络，这是完全可行的。那么以上便是本节课的全部内容，感谢大家的收看。
